{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de una Red neuronal Pequeña.\n",
    "Código Basado en https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from random import seed, random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una red neuronal es una función universal con  3 tipos de capas: capa de entrada, capas intermedia capa de salida. \n",
    "\n",
    "* Un problema es que la derivada no se puede calcular directamente. A partir de la capa de salida, no se tiene acceso a las derivadas de las capas intermedias.\n",
    "\n",
    "* Por lo tanto se requiere un algoritmo especial para calcular la derivada basada en la regla de la cadena de cálculo que indica que si se tienen funciones anidadas: $y=g(x), z = f(y)$ o bien $z = f(g(x))$, una manera de resolver esta anidación es aplicando la regla de la cadena. Ejemplo:\n",
    "\n",
    "$$\\frac{d z}{d x} = \\frac{dz}{dy} \\frac{dy}{dx} $$\n",
    "\n",
    "* El algoritmo de **Propagación hacia atrás** (Backpropagation, BP), es un algoritmo de entrenamiento supervisado permite calcular la derivada de una red neuronal y por lo tanto actualizar los pesos de todas las capas intermedias.\n",
    "\n",
    "* Por lo tanto describiremos un código de una red neuronal con fines didácticos. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo Red neuronal multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inicializando la red**\n",
    "\n",
    "initialize_network recibe el tamaño del vector de entrada, el número de capas ocultas y el numero de capas de salida.\n",
    "\n",
    "Si inicializamos initialize_network(3, 2, 2), tendremos 2 conjuntos de 3+1 pesos considerando el *bias* para la primera capa que asocia cada entrada de $[x_0,x_1,x_0]$ con cada neurona intermedia. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capa de entrada: [{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}, {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}]\n",
      "capa de salida: [{'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}, {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(2, 2, 2)\n",
    "\n",
    "print(\"capa de entrada:\", network[0])\n",
    "print(\"capa de salida:\", network[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]},\n",
       " {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Propagación hacia adelante** (Forwad propagation): salida de la red neuronal dada una entrada $\\mathbf{x}$\n",
    "\n",
    "Es la función que nos permite utilizar la red neuronal para hacer clasificación. La entrada se va procesando a través de las capas hasta obtener una salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activación de la neurona**\n",
    "\n",
    "Es el cálculo de una regresión lineal de la forma \n",
    "\n",
    "$$y = \\mathbf{x}\\mathbf{w}^T + w_0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calcular activacion de la neurona por una entrada\n",
    "def activate(weights, inputs):\n",
    "    # W*X+w_0\n",
    "    \n",
    "    #bias: w_0\n",
    "    activation = weights[-1]\n",
    "    \n",
    "    #calcular w_0 + W*X\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "        \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transferencia de la neurona**\n",
    "\n",
    "Existen muchas funciones de transferencia como relu, tanh identidad, pero una de las mas utilizadas es la función sigmoide con forma de s o también llamada curva logística. Tiene la forma:\n",
    "\n",
    "$$x = 1/(1+e^{-y}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurona\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Propagación hacia adelante**\n",
    "\n",
    "Es la red neuronal en funcionamiento. Recibe una entrada $\\mathbf{x_i}$ y regresa una salida $y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, inputs):\n",
    "\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivada de transferencia hacia atrás de las capas intermedias**\n",
    "\n",
    "Esta derivada es solo una parte de la derivda de toda la red neuronal que se hace en forma encadenada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta es la derivada de la función sigmoide\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Propagación hacia atrás del error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# Propaga el error hacia atrás y lo va guardando en la estructura de la red\n",
    "def backward_propagate_error(network, expected):\n",
    "    \n",
    "    #comenzamos con la capa de salida\n",
    "    for i in reversed(range(len(network))):\n",
    "        \n",
    "        # por cada capa de la red asignada a layer\n",
    "        layer = network[i]\n",
    "        \n",
    "        #el tamaño de la capa es equivalente al numero de neuronas en la capa\n",
    "        n_neuronas = len(layer)\n",
    "        \n",
    "        #inicializamos error\n",
    "        errors = list()\n",
    "        \n",
    "        # la primera vez calculamos sobre la ultima capa que es a la que tenemos acceso en un inicio\n",
    "        if i == len(network)-1:\n",
    "            \n",
    "            # por cada neurona de la capa hacer:\n",
    "            for j in range(n_neuronas):\n",
    "            \n",
    "                neuron = layer[j]\n",
    "\n",
    "                yhat = neuron['output']\n",
    "                y = expected[j]\n",
    "            \n",
    "                #esta es la derivada de la función de costo basada en la log verosimilitud\n",
    "                # LL(\\theta) = (y log yhat + (1-y) log [1-yhat])\n",
    "                # Aqui comienza la transferencia\n",
    "                e = (yhat-y)/((yhat-1)*yhat)\n",
    "                errors.append(e)\n",
    "        \n",
    "        else:\n",
    "            #aqui son las capas restantes hacia atrás donde propagamos el erro\n",
    "            \n",
    "            for j in range(n_neuronas):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "                \n",
    "        #aqui guardamos todos los delta asociada a cada neurona de cada capa\n",
    "        for j in range(n_neuronas):\n",
    "            neuron = layer[j]\n",
    "            \n",
    "            #el error magnifica el delta que depende de derivada de transferencia\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actualizar pesos con el error**\n",
    "\n",
    "Se recibe la red con los deltas, y un vector de entrada $row$ y la tasa de aprendizaje $l_rate$ asignada por el usuario la cual especifica en porcentaje de actualización que queremos para los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    \n",
    "    # por cada capa de la red\n",
    "    for i in range(len(network)):\n",
    "        \n",
    "        #extraer la clasificación real del vector de entrada. \n",
    "        inputs = row[:-1]\n",
    "        \n",
    "        #las salidas ahora son las entradas de la siguiente capa\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    " \n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                \n",
    "                # por cada neurona y cada una de sus entradas\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            \n",
    "            #actualización del bias\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "           \n",
    "\n",
    "       \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenar la red con Gradiente Descendiente estocástico**\n",
    "\n",
    "* una característica diferente a Gradiente descendiente normal, es que por cada iteación, utiliza un vector de entrada diferente para actualizar los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        \n",
    "        #por cada vector en el conjunto de entrenamiento.\n",
    "        for row in train:\n",
    "            \n",
    "            # se calcular la salida de un elemento del vector\n",
    "            outputs = forward_propagate(network, row)\n",
    "            \n",
    "            #preparamos con ceros\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            \n",
    "            #la posición row[-1] \\in {0,1} tendrá un 1\n",
    "            expected[int(row[-1])] = 1\n",
    "            \n",
    "            #sumamos cada error en cada dimensión diferente\n",
    "            \n",
    "       #     sum_error += sum([(expected[i]-outputs[i] )**2 for i in range(len(expected))])\n",
    "            \n",
    "            zi = expected[1]\n",
    "            zi_e = outputs[1]\n",
    "            \n",
    "            #aquí el error reportado es la suma log verosimilitud , queremos que sea el máximo\n",
    "            #sum_error += zi*math.log(zi_e)+(1-zi)*math.log(1-zi_e)\n",
    "            sum_error += zi*math.log(zi_e)+(1-zi)*math.log(1-zi_e)\n",
    "        \n",
    "      \n",
    "            #propagamos hacia atrás \n",
    "            #el error generado por la diferencia de el valor esperado y el valor generado por la red\n",
    "            # se reajusta la red para predecir mejor el valor esperado\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = [[ 0.        , 43.57879291,  0.        ],\n",
    "       [ 1.        , 38.25686159,  0.        ],\n",
    "       [ 2.        , 33.65222425,  0.        ],\n",
    "       [ 3.        , 35.58326609,  0.        ],\n",
    "       [ 4.        , 34.71588722,  0.        ],\n",
    "       [ 5.        , 32.38702072,  0.        ],\n",
    "       [ 6.        , 28.39419039,  0.        ],\n",
    "       [ 7.        , 20.20082333,  0.        ],\n",
    "       [ 8.        , 24.19586975,  0.        ],\n",
    "       [ 9.        , 18.48536939,  0.        ],\n",
    "       [10.        , 19.43098795,  0.        ],\n",
    "       [11.        ,  9.78931084,  0.        ],\n",
    "       [12.        , 14.4403072 ,  0.        ],\n",
    "       [13.        ,  8.95888303,  0.        ],\n",
    "       [14.        ,  7.17704765,  0.        ],\n",
    "       [15.        ,  9.57779397,  0.        ],\n",
    "       [16.        ,  7.58424501,  0.        ],\n",
    "       [17.        ,  8.9341588 ,  0.        ],\n",
    "       [18.        ,  3.13378008,  0.        ],\n",
    "       [19.        ,  9.23532406,  0.        ],\n",
    "       [20.        ,  6.43370902,  0.        ],\n",
    "       [21.        ,  6.67961846,  0.        ],\n",
    "       [22.        ,  5.15107388,  0.        ],\n",
    "       [23.        ,  3.13252828,  0.        ],\n",
    "       [24.        ,  9.40036651,  0.        ],\n",
    "       [25.        ,  8.96646931,  0.        ],\n",
    "       [26.        , 12.97187713,  0.        ],\n",
    "       [27.        ,  6.17097537,  0.        ],\n",
    "       [28.        , 14.94380716,  0.        ],\n",
    "       [29.        , 15.78038035,  0.        ],\n",
    "       [30.        , 13.44006058,  0.        ],\n",
    "       [31.        , 12.38640541,  0.        ],\n",
    "       [32.        , 18.24681531,  0.        ],\n",
    "       [33.        , 24.89568658,  0.        ],\n",
    "       [34.        , 20.9740082 ,  0.        ],\n",
    "       [35.        , 29.60592301,  0.        ],\n",
    "       [36.        , 28.15828701,  0.        ],\n",
    "       [37.        , 36.2793585 ,  0.        ],\n",
    "       [38.        , 35.14373666,  0.        ],\n",
    "       [39.        , 45.91909167,  0.        ],\n",
    "       [ 0.        , 72.99963497,  1.        ],\n",
    "       [ 1.        , 66.25249531,  1.        ],\n",
    "       [ 2.        , 69.47862325,  1.        ],\n",
    "       [ 3.        , 68.12423544,  1.        ],\n",
    "       [ 4.        , 58.18406218,  1.        ],\n",
    "       [ 5.        , 59.71572434,  1.        ],\n",
    "       [ 6.        , 56.45457802,  1.        ],\n",
    "       [ 7.        , 52.4068255 ,  1.        ],\n",
    "       [ 8.        , 47.24274882,  1.        ],\n",
    "       [ 9.        , 43.36311311,  1.        ],\n",
    "       [10.        , 44.07483255,  1.        ],\n",
    "       [11.        , 39.85302512,  1.        ],\n",
    "       [12.        , 44.01192218,  1.        ],\n",
    "       [13.        , 41.07318483,  1.        ],\n",
    "       [14.        , 33.8127034 ,  1.        ],\n",
    "       [15.        , 33.21045858,  1.        ],\n",
    "       [16.        , 37.15741812,  1.        ],\n",
    "       [17.        , 31.73730861,  1.        ],\n",
    "       [18.        , 34.83305033,  1.        ],\n",
    "       [19.        , 39.87419754,  1.        ],\n",
    "       [20.        , 31.48844934,  1.        ],\n",
    "       [21.        , 35.0882841 ,  1.        ],\n",
    "       [22.        , 39.71262275,  1.        ],\n",
    "       [23.        , 31.62777132,  1.        ],\n",
    "       [24.        , 40.68066481,  1.        ],\n",
    "       [25.        , 36.14619892,  1.        ],\n",
    "       [26.        , 38.39007043,  1.        ],\n",
    "       [27.        , 35.30625817,  1.        ],\n",
    "       [28.        , 38.9855814 ,  1.        ],\n",
    "       [29.        , 44.46375169,  1.        ],\n",
    "       [30.        , 42.9567545 ,  1.        ],\n",
    "       [31.        , 43.13876601,  1.        ],\n",
    "       [32.        , 46.36785753,  1.        ],\n",
    "       [33.        , 53.20209487,  1.        ],\n",
    "       [34.        , 54.57671019,  1.        ],\n",
    "       [35.        , 61.72180758,  1.        ],\n",
    "       [36.        , 56.20328364,  1.        ],\n",
    "       [37.        , 60.75140654,  1.        ],\n",
    "       [38.        , 62.61155513,  1.        ],\n",
    "       [39.        , 67.27858905,  1.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.59697275, 0.        ],\n",
       "       [0.02564103, 0.52406922, 0.        ],\n",
       "       [0.05128205, 0.46099168, 0.        ],\n",
       "       [0.07692308, 0.48744444, 0.        ],\n",
       "       [0.1025641 , 0.47556248, 0.        ],\n",
       "       [0.12820513, 0.44366004, 0.        ],\n",
       "       [0.15384615, 0.38896346, 0.        ],\n",
       "       [0.17948718, 0.27672499, 0.        ],\n",
       "       [0.20512821, 0.33145193, 0.        ],\n",
       "       [0.23076923, 0.2532255 , 0.        ],\n",
       "       [0.25641026, 0.26617925, 0.        ],\n",
       "       [0.28205128, 0.13410082, 0.        ],\n",
       "       [0.30769231, 0.19781342, 0.        ],\n",
       "       [0.33333333, 0.12272504, 0.        ],\n",
       "       [0.35897436, 0.09831621, 0.        ],\n",
       "       [0.38461538, 0.13120331, 0.        ],\n",
       "       [0.41025641, 0.10389429, 0.        ],\n",
       "       [0.43589744, 0.12238635, 0.        ],\n",
       "       [0.46153846, 0.04292871, 0.        ],\n",
       "       [0.48717949, 0.12651192, 0.        ],\n",
       "       [0.51282051, 0.08813344, 0.        ],\n",
       "       [0.53846154, 0.09150208, 0.        ],\n",
       "       [0.56410256, 0.07056301, 0.        ],\n",
       "       [0.58974359, 0.04291156, 0.        ],\n",
       "       [0.61538462, 0.12877279, 0.        ],\n",
       "       [0.64102564, 0.12282896, 0.        ],\n",
       "       [0.66666667, 0.17769784, 0.        ],\n",
       "       [0.69230769, 0.08453433, 0.        ],\n",
       "       [0.71794872, 0.20471071, 0.        ],\n",
       "       [0.74358974, 0.21617067, 0.        ],\n",
       "       [0.76923077, 0.18411134, 0.        ],\n",
       "       [0.79487179, 0.16967763, 0.        ],\n",
       "       [0.82051282, 0.24995762, 0.        ],\n",
       "       [0.84615385, 0.34103851, 0.        ],\n",
       "       [0.87179487, 0.28731662, 0.        ],\n",
       "       [0.8974359 , 0.40556262, 0.        ],\n",
       "       [0.92307692, 0.38573189, 0.        ],\n",
       "       [0.94871795, 0.49698   , 0.        ],\n",
       "       [0.97435897, 0.48142346, 0.        ],\n",
       "       [1.        , 0.6290318 , 0.        ],\n",
       "       [0.        , 1.        , 1.        ],\n",
       "       [0.02564103, 0.90757297, 1.        ],\n",
       "       [0.05128205, 0.95176672, 1.        ],\n",
       "       [0.07692308, 0.93321337, 1.        ],\n",
       "       [0.1025641 , 0.79704593, 1.        ],\n",
       "       [0.12820513, 0.81802771, 1.        ],\n",
       "       [0.15384615, 0.77335425, 1.        ],\n",
       "       [0.17948718, 0.71790531, 1.        ],\n",
       "       [0.20512821, 0.64716418, 1.        ],\n",
       "       [0.23076923, 0.59401822, 1.        ],\n",
       "       [0.25641026, 0.60376785, 1.        ],\n",
       "       [0.28205128, 0.54593458, 1.        ],\n",
       "       [0.30769231, 0.60290606, 1.        ],\n",
       "       [0.33333333, 0.56264918, 1.        ],\n",
       "       [0.35897436, 0.46319003, 1.        ],\n",
       "       [0.38461538, 0.45494006, 1.        ],\n",
       "       [0.41025641, 0.50900827, 1.        ],\n",
       "       [0.43589744, 0.43475983, 1.        ],\n",
       "       [0.46153846, 0.47716746, 1.        ],\n",
       "       [0.48717949, 0.54622462, 1.        ],\n",
       "       [0.51282051, 0.43135078, 1.        ],\n",
       "       [0.53846154, 0.48066383, 1.        ],\n",
       "       [0.56410256, 0.54401125, 1.        ],\n",
       "       [0.58974359, 0.43325931, 1.        ],\n",
       "       [0.61538462, 0.55727217, 1.        ],\n",
       "       [0.64102564, 0.49515589, 1.        ],\n",
       "       [0.66666667, 0.52589401, 1.        ],\n",
       "       [0.69230769, 0.48364979, 1.        ],\n",
       "       [0.71794872, 0.53405173, 1.        ],\n",
       "       [0.74358974, 0.60909553, 1.        ],\n",
       "       [0.76923077, 0.58845163, 1.        ],\n",
       "       [0.79487179, 0.59094496, 1.        ],\n",
       "       [0.82051282, 0.63517931, 1.        ],\n",
       "       [0.84615385, 0.72879946, 1.        ],\n",
       "       [0.87179487, 0.74762991, 1.        ],\n",
       "       [0.8974359 , 0.84550844, 1.        ],\n",
       "       [0.92307692, 0.76991184, 1.        ],\n",
       "       [0.94871795, 0.83221521, 1.        ],\n",
       "       [0.97435897, 0.85769682, 1.        ],\n",
       "       [1.        , 0.92162912, 1.        ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array(dataset)\n",
    "\n",
    "dataset[:,0] = dataset[:,0]/max(dataset[:,0])\n",
    "dataset[:,1] = dataset[:,1]/max(dataset[:,1])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  2\n",
      "outputs:  2\n",
      ">epoch=0, lrate=0.010, error=-69.884\n",
      ">epoch=1, lrate=0.010, error=-62.512\n",
      ">epoch=2, lrate=0.010, error=-58.938\n",
      ">epoch=3, lrate=0.010, error=-57.196\n",
      ">epoch=4, lrate=0.010, error=-56.314\n",
      ">epoch=5, lrate=0.010, error=-55.845\n",
      ">epoch=6, lrate=0.010, error=-55.580\n",
      ">epoch=7, lrate=0.010, error=-55.422\n",
      ">epoch=8, lrate=0.010, error=-55.322\n",
      ">epoch=9, lrate=0.010, error=-55.254\n",
      ">epoch=10, lrate=0.010, error=-55.206\n",
      ">epoch=11, lrate=0.010, error=-55.169\n",
      ">epoch=12, lrate=0.010, error=-55.139\n",
      ">epoch=13, lrate=0.010, error=-55.113\n",
      ">epoch=14, lrate=0.010, error=-55.089\n",
      ">epoch=15, lrate=0.010, error=-55.066\n",
      ">epoch=16, lrate=0.010, error=-55.045\n",
      ">epoch=17, lrate=0.010, error=-55.023\n",
      ">epoch=18, lrate=0.010, error=-55.001\n",
      ">epoch=19, lrate=0.010, error=-54.978\n",
      ">epoch=20, lrate=0.010, error=-54.956\n",
      ">epoch=21, lrate=0.010, error=-54.932\n",
      ">epoch=22, lrate=0.010, error=-54.908\n",
      ">epoch=23, lrate=0.010, error=-54.883\n",
      ">epoch=24, lrate=0.010, error=-54.858\n",
      ">epoch=25, lrate=0.010, error=-54.832\n",
      ">epoch=26, lrate=0.010, error=-54.804\n",
      ">epoch=27, lrate=0.010, error=-54.776\n",
      ">epoch=28, lrate=0.010, error=-54.747\n",
      ">epoch=29, lrate=0.010, error=-54.717\n",
      ">epoch=30, lrate=0.010, error=-54.686\n",
      ">epoch=31, lrate=0.010, error=-54.653\n",
      ">epoch=32, lrate=0.010, error=-54.620\n",
      ">epoch=33, lrate=0.010, error=-54.585\n",
      ">epoch=34, lrate=0.010, error=-54.549\n",
      ">epoch=35, lrate=0.010, error=-54.512\n",
      ">epoch=36, lrate=0.010, error=-54.473\n",
      ">epoch=37, lrate=0.010, error=-54.433\n",
      ">epoch=38, lrate=0.010, error=-54.392\n",
      ">epoch=39, lrate=0.010, error=-54.348\n",
      ">epoch=40, lrate=0.010, error=-54.304\n",
      ">epoch=41, lrate=0.010, error=-54.257\n",
      ">epoch=42, lrate=0.010, error=-54.209\n",
      ">epoch=43, lrate=0.010, error=-54.159\n",
      ">epoch=44, lrate=0.010, error=-54.107\n",
      ">epoch=45, lrate=0.010, error=-54.053\n",
      ">epoch=46, lrate=0.010, error=-53.997\n",
      ">epoch=47, lrate=0.010, error=-53.939\n",
      ">epoch=48, lrate=0.010, error=-53.879\n",
      ">epoch=49, lrate=0.010, error=-53.816\n",
      ">epoch=50, lrate=0.010, error=-53.751\n",
      ">epoch=51, lrate=0.010, error=-53.683\n",
      ">epoch=52, lrate=0.010, error=-53.613\n",
      ">epoch=53, lrate=0.010, error=-53.540\n",
      ">epoch=54, lrate=0.010, error=-53.465\n",
      ">epoch=55, lrate=0.010, error=-53.386\n",
      ">epoch=56, lrate=0.010, error=-53.305\n",
      ">epoch=57, lrate=0.010, error=-53.220\n",
      ">epoch=58, lrate=0.010, error=-53.132\n",
      ">epoch=59, lrate=0.010, error=-53.041\n",
      ">epoch=60, lrate=0.010, error=-52.946\n",
      ">epoch=61, lrate=0.010, error=-52.848\n",
      ">epoch=62, lrate=0.010, error=-52.747\n",
      ">epoch=63, lrate=0.010, error=-52.641\n",
      ">epoch=64, lrate=0.010, error=-52.532\n",
      ">epoch=65, lrate=0.010, error=-52.418\n",
      ">epoch=66, lrate=0.010, error=-52.301\n",
      ">epoch=67, lrate=0.010, error=-52.179\n",
      ">epoch=68, lrate=0.010, error=-52.053\n",
      ">epoch=69, lrate=0.010, error=-51.922\n",
      ">epoch=70, lrate=0.010, error=-51.787\n",
      ">epoch=71, lrate=0.010, error=-51.648\n",
      ">epoch=72, lrate=0.010, error=-51.503\n",
      ">epoch=73, lrate=0.010, error=-51.354\n",
      ">epoch=74, lrate=0.010, error=-51.200\n",
      ">epoch=75, lrate=0.010, error=-51.041\n",
      ">epoch=76, lrate=0.010, error=-50.877\n",
      ">epoch=77, lrate=0.010, error=-50.707\n",
      ">epoch=78, lrate=0.010, error=-50.533\n",
      ">epoch=79, lrate=0.010, error=-50.353\n",
      ">epoch=80, lrate=0.010, error=-50.168\n",
      ">epoch=81, lrate=0.010, error=-49.977\n",
      ">epoch=82, lrate=0.010, error=-49.782\n",
      ">epoch=83, lrate=0.010, error=-49.580\n",
      ">epoch=84, lrate=0.010, error=-49.374\n",
      ">epoch=85, lrate=0.010, error=-49.162\n",
      ">epoch=86, lrate=0.010, error=-48.945\n",
      ">epoch=87, lrate=0.010, error=-48.723\n",
      ">epoch=88, lrate=0.010, error=-48.496\n",
      ">epoch=89, lrate=0.010, error=-48.263\n",
      ">epoch=90, lrate=0.010, error=-48.026\n",
      ">epoch=91, lrate=0.010, error=-47.783\n",
      ">epoch=92, lrate=0.010, error=-47.536\n",
      ">epoch=93, lrate=0.010, error=-47.285\n",
      ">epoch=94, lrate=0.010, error=-47.028\n",
      ">epoch=95, lrate=0.010, error=-46.768\n",
      ">epoch=96, lrate=0.010, error=-46.503\n",
      ">epoch=97, lrate=0.010, error=-46.235\n",
      ">epoch=98, lrate=0.010, error=-45.962\n",
      ">epoch=99, lrate=0.010, error=-45.686\n",
      "[{'weights': [0.5814385824642848, -1.5008435761580656, 0.20526225553955768], 'output': 0.3562651104886004, 'delta': -0.17226317933734275}, {'weights': [0.08554377980901433, 1.8022172637890805, -0.38715755741745445], 'output': 0.7950850614186238, 'delta': 0.12311296192828829}, {'weights': [-0.15226928765405226, 1.8166480463224348, -0.24029814052409232], 'output': 0.7821406545366543, 'delta': 0.12365288877980708}]\n",
      "[{'weights': [1.12970606827526, -1.4796758950415088, -0.8774713507602847, 0.6547913197982277], 'output': 0.31042171810655145, 'delta': -0.31042171810655145}, {'weights': [-1.3158826164162045, 0.9803080651088831, 1.496917617725366, -0.6449635094473668], 'output': 0.6961982748337017, 'delta': 0.30380172516629833}]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = len(dataset[0]) - 1\n",
    "print(\"inputs: \", n_inputs)\n",
    "\n",
    "#total possible number of output values\n",
    "n_outputs = len(set([ int(row[-1]) for row in dataset]))\n",
    "print(\"outputs: \", n_outputs)\n",
    "\n",
    "network = initialize_network(n_inputs, 3, n_outputs)\n",
    "train_network(network, dataset, 0.01, 100, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 0\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 0.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n",
      "expectation: 1.0 reality 1\n"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "for x in dataset:\n",
    "    \n",
    "    y = forward_propagate(network, x[:2])\n",
    "    print(\"expectation:\",x[-1],\"reality\",1*((y[-1])>0.5))\n",
    "    \n",
    "    e+= (x[-1]- 1*((y[-1])>0.5))**2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j=0\n",
    "i=0\n",
    "\n",
    "n_neuronas = len(network[i])\n",
    "\n",
    "for j in range(n_neuronas):\n",
    "    print(\"neurna no.\",j)\n",
    "    error = 0\n",
    "\n",
    "    for neuron in network[i+1]:\n",
    "   #     print(\"layer:\",neuron)\n",
    "   #     print(\"weights\",i+1,j,neuron[\"weights\"][j])\n",
    "        error += neuron[\"weights\"][j]*neuron['delta']\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
